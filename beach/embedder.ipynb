{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Item:\n",
    "    item_id: int\n",
    "    content: str\n",
    "    anchor: str\n",
    "    entailment: str\n",
    "    contradiction: str\n",
    "    irrelevance: str\n",
    "    subject: List[str]\n",
    "\n",
    "    def __init__(self, item_id: int, json_str: str) -> None:\n",
    "        obj = json.loads(json_str)\n",
    "        self.item_id = item_id\n",
    "        self.content = obj[\"content\"]\n",
    "        self.anchor = obj[\"passage\"][\"anchor\"][0]\n",
    "        self.entailment = obj[\"passage\"][\"entailment\"][0]\n",
    "        self.contradiction = obj[\"passage\"][\"contradiction\"][0]\n",
    "        self.irrelevance = obj[\"passage\"][\"irrelevance\"][0]\n",
    "        self.subject = list(map(self._process_item_text, obj[\"passage\"][\"subject\"]))\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_items() -> List[\"Item\"]:\n",
    "        items: List[Item] = []\n",
    "        for item_file_path in sorted(glob.glob(\"./data/*.json\")):\n",
    "            with open(item_file_path) as item_file:\n",
    "                item_id = int(Path(item_file_path).stem)\n",
    "                items.append(Item(item_id, item_file.read()))\n",
    "        return items\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_item_text(item_text: str) -> str:\n",
    "        return re.sub(\"^(\\d+\\.|-|\\*)\", \"\", item_text.strip()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "def generate_data(items: List[Item], train_ratio: float) -> Tuple[List[Item], List[Item], List[InputExample], List[Tuple[str, ...]]]:\n",
    "    # Prepare primary items (items with subjects) and secondary items (items without subjects)\n",
    "    primary_items: List[Item] = []\n",
    "    secondary_items: List[Item] = []\n",
    "    for item in items:\n",
    "        if len(item.subject) > 0:\n",
    "            primary_items.append(item)\n",
    "        else:\n",
    "            secondary_items.append(item)\n",
    "    # Prepare train and val data\n",
    "    train_data_len = int(train_ratio * len(primary_items))\n",
    "    train_items = primary_items[:train_data_len] + secondary_items\n",
    "    val_items = primary_items[train_data_len:]\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    for item in train_items:\n",
    "        train_data.append(InputExample(texts=[item.content, item.anchor]))\n",
    "    for item in val_items:\n",
    "        val_data.append((item.content, item.anchor))\n",
    "    return train_items, val_items, train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class SimilarityEvaluator():\n",
    "    model: SentenceTransformer\n",
    "    index: faiss.IndexFlatIP\n",
    "    queries: List[List[str]]\n",
    "\n",
    "    def __init__(self, model: SentenceTransformer, items: List[Item]):\n",
    "        sentences = [item.content for item in items]\n",
    "        sentence_embeddings = model.encode(sentences)\n",
    "        faiss.normalize_L2(sentence_embeddings)\n",
    "        _, size = sentence_embeddings.shape\n",
    "        self.model = model\n",
    "        self.index = faiss.IndexFlatIP(size)\n",
    "        self.index.add(sentence_embeddings)\n",
    "        self.queries = [item.subject for item in items]\n",
    "\n",
    "    def search(self, queries: List[str], limit: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if limit is None:\n",
    "            limit = self.index.ntotal\n",
    "        item_query_embeddings = self.model.encode(queries)\n",
    "        faiss.normalize_L2(item_query_embeddings)\n",
    "        similarities, indices = self.index.search(item_query_embeddings, limit)\n",
    "        return similarities, indices\n",
    "\n",
    "    def calc_avg_rank(self) -> float:\n",
    "        count = 0\n",
    "        index_sum = 0\n",
    "        for i, item_queries in enumerate(self.queries):\n",
    "            _similarities, indices = self.search(item_queries)\n",
    "            _hit_subject, hit_indices = np.asarray(indices == i).nonzero()\n",
    "            index_sum += hit_indices.sum()\n",
    "            count += len(hit_indices)\n",
    "        return index_sum / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "\n",
    "def fine_tune(\n",
    "    model: SentenceTransformer, train_data: List[InputExample], val_data: List[Tuple[str, ...]],\n",
    "    output_path: str, batch_size: int, epochs: int,\n",
    "):\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    train_loss = MultipleNegativesRankingLoss(model)\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=int(len(train_dataloader) * epochs * 0.1),\n",
    "        output_path=output_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.5\n",
    "\n",
    "_train_items, val_items, train_data, val_data = generate_data(Item.fetch_items(), TRAIN_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "OUTPUT_PATH = \"./embedder/\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "\n",
    "model = SentenceTransformer(MODEL_ID)\n",
    "shutil.rmtree(OUTPUT_PATH, ignore_errors=True)\n",
    "old_avg_rank = SimilarityEvaluator(model, val_items).calc_avg_rank()\n",
    "fine_tune(model, train_data, val_data, OUTPUT_PATH, BATCH_SIZE, EPOCHS)\n",
    "new_avg_rank = SimilarityEvaluator(model, val_items).calc_avg_rank()\n",
    "print(f\"Average rank (lower is better): old={old_avg_rank}, new={new_avg_rank}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
