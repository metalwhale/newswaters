{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Item:\n",
    "    item_id: int\n",
    "    content: str\n",
    "    anchor: str\n",
    "    entailment: str\n",
    "    contradiction: str\n",
    "    irrelevance: str\n",
    "    subject: List[str]\n",
    "\n",
    "    def __init__(self, item_id: int, json_str: str) -> None:\n",
    "        obj = json.loads(json_str)\n",
    "        self.item_id = item_id\n",
    "        self.content = obj[\"content\"]\n",
    "        self.anchor = obj[\"passage\"][\"anchor\"][0]\n",
    "        self.entailment = obj[\"passage\"][\"entailment\"][0]\n",
    "        self.contradiction = obj[\"passage\"][\"contradiction\"][0]\n",
    "        self.irrelevance = obj[\"passage\"][\"irrelevance\"][0]\n",
    "        self.subject = list(map(self._process_item_text, obj[\"passage\"][\"subject\"]))\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_items(limit: int) -> List[\"Item\"]:\n",
    "        items: List[Item] = []\n",
    "        for item_file_path in sorted(glob.glob(\"./data/*.json\"))[:limit]:\n",
    "            with open(item_file_path) as item_file:\n",
    "                item_id = int(Path(item_file_path).stem)\n",
    "                items.append(Item(item_id, item_file.read()))\n",
    "        return items\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_item_text(item_text: str) -> str:\n",
    "        return re.sub(\"^(\\d+\\.|-|\\*)\", \"\", item_text.strip()).strip()\n",
    "\n",
    "\n",
    "items = Item.fetch_items(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "def generate_data(\n",
    "    items: List[Item], queryable_train_ratio: float,\n",
    ") -> Tuple[List[Item], List[Item], List[InputExample], List[Tuple[str, ...]]]:\n",
    "    queryable_items: List[Item] = []\n",
    "    unqueryable_items: List[Item] = []\n",
    "    for item in items:\n",
    "        if len(item.subject) > 0:\n",
    "            queryable_items.append(item)\n",
    "        else:\n",
    "            unqueryable_items.append(item)\n",
    "    # Prepare train and val data\n",
    "    queryable_train_data_len = int(queryable_train_ratio * len(queryable_items))\n",
    "    train_items = queryable_items[:queryable_train_data_len] + unqueryable_items\n",
    "    val_items = queryable_items[queryable_train_data_len:]\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    for item in train_items:\n",
    "        train_data.append(InputExample(texts=[item.content, item.anchor]))\n",
    "    for item in val_items:\n",
    "        val_data.append((item.content, item.anchor))\n",
    "    return train_items, val_items, train_data, val_data\n",
    "\n",
    "\n",
    "QUERYABLE_TRAIN_RATIO = 0.9\n",
    "\n",
    "_train_items, val_items, train_data, _val_data = generate_data(items, QUERYABLE_TRAIN_RATIO)\n",
    "print(\"Train data length:\", len(train_data))\n",
    "print(\"Val items length:\", len(val_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from typing import Optional\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "\n",
    "class AvgRankCalculator:\n",
    "    model: SentenceTransformer\n",
    "    index: faiss.IndexFlatIP\n",
    "    queries: List[List[str]]\n",
    "\n",
    "    def __init__(self, model: SentenceTransformer, items: List[Item]):\n",
    "        sentences = [item.content for item in items]\n",
    "        sentence_embeddings = model.encode(sentences)\n",
    "        faiss.normalize_L2(sentence_embeddings)\n",
    "        _, size = sentence_embeddings.shape\n",
    "        self.model = model\n",
    "        self.index = faiss.IndexFlatIP(size)\n",
    "        self.index.add(sentence_embeddings)\n",
    "        self.queries = [item.subject for item in items]\n",
    "\n",
    "    def search(self, queries: List[str], limit: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if limit is None:\n",
    "            limit = self.index.ntotal\n",
    "        item_query_embeddings = self.model.encode(queries)\n",
    "        faiss.normalize_L2(item_query_embeddings)\n",
    "        similarities, indices = self.index.search(item_query_embeddings, limit)\n",
    "        return similarities, indices\n",
    "\n",
    "    def calc_avg_rank(self) -> float:\n",
    "        count = 0\n",
    "        index_sum = 0\n",
    "        for i, item_queries in enumerate(self.queries):\n",
    "            _similarities, indices = self.search(item_queries)\n",
    "            _hit_subject, hit_indices = np.asarray(indices == i).nonzero()\n",
    "            index_sum += hit_indices.sum()\n",
    "            count += len(hit_indices)\n",
    "        return index_sum / count\n",
    "\n",
    "\n",
    "class AvgRankEvaluator(SentenceEvaluator):\n",
    "    items: List[Item]\n",
    "\n",
    "    def __init__(self, items: List[Item]):\n",
    "        self.items = items\n",
    "\n",
    "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "        rank = AvgRankCalculator(model, self.items).calc_avg_rank()\n",
    "        print(rank)\n",
    "        return rank\n",
    "\n",
    "\n",
    "def fine_tune(\n",
    "    model: SentenceTransformer, train_data: List[InputExample], val_items: List[Item],\n",
    "    output_path: str, batch_size: int, epochs: int,\n",
    "):\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    train_loss = MultipleNegativesRankingLoss(model)\n",
    "    evaluator = AvgRankEvaluator(val_items)\n",
    "    evaluator(model)\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=int(len(train_dataloader) * epochs * 0.1),\n",
    "        evaluator=evaluator,\n",
    "        output_path=output_path,\n",
    "    )\n",
    "\n",
    "\n",
    "# MODEL_ID = \"all-mpnet-base-v2\"\n",
    "# MODEL_ID = \"all-distilroberta-v1\"\n",
    "MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# MODEL_ID = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "OUTPUT_PATH = \"./embedder/\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 2\n",
    "\n",
    "model = SentenceTransformer(MODEL_ID)\n",
    "shutil.rmtree(OUTPUT_PATH, ignore_errors=True)\n",
    "for e in range(EPOCHS):\n",
    "    print(f\"EPOCH {e}\")\n",
    "    for i in range(0, len(train_data), len(train_data) // 10):\n",
    "        train_data_chunk = train_data[i:i + len(train_data) // 10]\n",
    "        fine_tune(model, train_data_chunk, val_items, OUTPUT_PATH, BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_items = [item for item in items if len(item.subject) > 0]  # queryable_items\n",
    "old_calculator = AvgRankCalculator(SentenceTransformer(MODEL_ID), exam_items)\n",
    "new_calculator = AvgRankCalculator(model, exam_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old avg rank:\", old_calculator.calc_avg_rank())\n",
    "print(\"New avg rank:\", new_calculator.calc_avg_rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(exam_items):\n",
    "    query = item.subject[0]\n",
    "    if len(query.split(\" \")) > 5:\n",
    "        continue\n",
    "    _old_similarities, old_indices = old_calculator.search([query], limit=10)\n",
    "    _new_similarities, new_indices = new_calculator.search([query], limit=10)\n",
    "    old_indices, new_indices = old_indices[0], new_indices[0]\n",
    "    old_find = np.asarray(old_indices == i).nonzero()[0]\n",
    "    new_find = np.asarray(new_indices == i).nonzero()[0]\n",
    "    old_index = None if old_find.size == 0 else old_find[0]\n",
    "    new_index = None if new_find.size == 0 else new_find[0]\n",
    "    if (old_index is None and new_index is not None) \\\n",
    "            or (old_index is not None and new_index is not None and old_index - new_index > 5):\n",
    "        print(i, exam_items[i].item_id, old_index, new_index)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
