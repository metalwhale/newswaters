{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Item:\n",
    "    item_id: int\n",
    "    content: str\n",
    "    anchor: str\n",
    "    entailment: str\n",
    "    contradiction: str\n",
    "    irrelevance: str\n",
    "    subject: List[str]\n",
    "\n",
    "    def __init__(self, item_id: int, json_str: str) -> None:\n",
    "        obj = json.loads(json_str)\n",
    "        self.item_id = item_id\n",
    "        self.content = obj[\"content\"]\n",
    "        self.anchor = obj[\"passage\"][\"anchor\"][0]\n",
    "        self.entailment = obj[\"passage\"][\"entailment\"][0]\n",
    "        self.contradiction = obj[\"passage\"][\"contradiction\"][0]\n",
    "        self.irrelevance = obj[\"passage\"][\"irrelevance\"][0]\n",
    "        self.subject = list(map(self._process_item_text, obj[\"passage\"][\"subject\"]))\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_items() -> List[\"Item\"]:\n",
    "        items: List[Item] = []\n",
    "        for item_file_path in glob.glob(\"./data/*.json\"):\n",
    "            with open(item_file_path) as item_file:\n",
    "                item_id = int(Path(item_file_path).stem)\n",
    "                items.append(Item(item_id, item_file.read()))\n",
    "        return items\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_item_text(item_text: str) -> str:\n",
    "        return re.sub(\"^(\\d+\\.|-|\\*)\", \"\", item_text.strip()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "def generate_data(items: List[Item], train_ratio: float) -> Tuple[List[Item], List[Item], List[InputExample], List[Tuple[str, ...]]]:\n",
    "    # Prepare primary items (items with subjects) and secondary items (items without subjects)\n",
    "    random.shuffle(items)\n",
    "    primary_items: List[Item] = []\n",
    "    secondary_items: List[Item] = []\n",
    "    for item in items:\n",
    "        if len(item.subject) > 0:\n",
    "            primary_items.append(item)\n",
    "        else:\n",
    "            secondary_items.append(item)\n",
    "    # Prepare train and val data\n",
    "    train_data_len = int(train_ratio * len(primary_items))\n",
    "    train_items = primary_items[:train_data_len] + secondary_items\n",
    "    val_items = primary_items[train_data_len:]\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    for item in train_items:\n",
    "        train_data.append(InputExample(texts=[item.anchor, item.entailment, item.contradiction]))\n",
    "        train_data.append(InputExample(texts=[item.anchor, item.entailment, item.irrelevance]))\n",
    "    for item in val_items:\n",
    "        val_data.append((item.anchor, item.entailment, item.contradiction))\n",
    "        val_data.append((item.anchor, item.entailment, item.irrelevance))\n",
    "    return train_items, val_items, train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sentence_transformers.evaluation.TripletEvaluator import (\n",
    "    SimilarityFunction,\n",
    "    paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    ")\n",
    "\n",
    "\n",
    "class CustomTripletEvaluator(TripletEvaluator):\n",
    "    # See: https://github.com/UKPLab/sentence-transformers/blob/v2.2.2/sentence_transformers/evaluation/TripletEvaluator.py\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        anchors: List[str],\n",
    "        positives: List[str],\n",
    "        negatives: List[str],\n",
    "        main_distance_function: SimilarityFunction,\n",
    "        loss_margin: float,\n",
    "        batch_size: int = 16,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            anchors, positives, negatives,\n",
    "            main_distance_function=main_distance_function, batch_size=batch_size,\n",
    "        )\n",
    "        self.loss_margin = loss_margin\n",
    "        self.csv_headers = [\"epoch\", \"steps\", \"val_accuracy\", \"val_loss\"]\n",
    "\n",
    "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "        embeddings_anchors = model.encode(\n",
    "            self.anchors, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True\n",
    "        )\n",
    "        embeddings_positives = model.encode(\n",
    "            self.positives, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True\n",
    "        )\n",
    "        embeddings_negatives = model.encode(\n",
    "            self.negatives, batch_size=self.batch_size, show_progress_bar=self.show_progress_bar, convert_to_numpy=True\n",
    "        )\n",
    "        triplets_count = 0\n",
    "        correct_triplets_count = 0\n",
    "        loss = 0\n",
    "        if self.main_distance_function == SimilarityFunction.MANHATTAN:\n",
    "            pos_distances = paired_manhattan_distances(embeddings_anchors, embeddings_positives)\n",
    "            neg_distances = paired_manhattan_distances(embeddings_anchors, embeddings_negatives)\n",
    "        if self.main_distance_function == SimilarityFunction.EUCLIDEAN:\n",
    "            pos_distances = paired_euclidean_distances(embeddings_anchors, embeddings_positives)\n",
    "            neg_distances = paired_euclidean_distances(embeddings_anchors, embeddings_negatives)\n",
    "        else:\n",
    "            pos_distances = paired_cosine_distances(embeddings_anchors, embeddings_positives)\n",
    "            neg_distances = paired_cosine_distances(embeddings_anchors, embeddings_negatives)\n",
    "        for pos_distance, neg_distance in zip(pos_distances, neg_distances):\n",
    "            triplets_count += 1\n",
    "            if pos_distance < neg_distance:\n",
    "                correct_triplets_count += 1\n",
    "            # See: https://github.com/UKPLab/sentence-transformers/blob/v2.2.2/sentence_transformers/losses/TripletLoss.py\n",
    "            loss += pos_distance - neg_distance + self.loss_margin\n",
    "        accuracy = correct_triplets_count / triplets_count\n",
    "        loss /= triplets_count\n",
    "        if output_path is not None and self.write_csv:\n",
    "            csv_path = os.path.join(output_path, self.csv_file)\n",
    "            if not os.path.isfile(csv_path):\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "                with open(csv_path, newline=\"\", mode=\"w\", encoding=\"utf-8\") as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    writer.writerow(self.csv_headers)\n",
    "                    writer.writerow([epoch, steps, accuracy, loss])\n",
    "                    csv_file.flush()\n",
    "            else:\n",
    "                with open(csv_path, newline=\"\", mode=\"a\", encoding=\"utf-8\") as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    writer.writerow([epoch, steps, accuracy, loss])\n",
    "                    csv_file.flush()\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "\n",
    "\n",
    "class SimilarityEvaluator(SentenceEvaluator):\n",
    "    index: faiss.IndexFlatIP\n",
    "    queries: List[List[str]]\n",
    "    limit: int\n",
    "\n",
    "    def __init__(self, model: SentenceTransformer, items: List[Item], limit: int):\n",
    "        sentences = [item.content for item in items]\n",
    "        sentence_embeddings = model.encode(sentences)\n",
    "        faiss.normalize_L2(sentence_embeddings)\n",
    "        _, size = sentence_embeddings.shape\n",
    "        self.index = faiss.IndexFlatIP(size)\n",
    "        self.index.add(sentence_embeddings)\n",
    "        self.queries = [item.subject for item in items]\n",
    "        self.limit = limit\n",
    "\n",
    "    def __call__(self, model: SentenceTransformer, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "        hit_count = 0\n",
    "        for i, item_queries in enumerate(self.queries):\n",
    "            item_query_embeddings = model.encode(item_queries)\n",
    "            faiss.normalize_L2(item_query_embeddings)\n",
    "            _distances, ids = self.index.search(item_query_embeddings, self.limit)\n",
    "            hit_subject, _hit_indices = np.asarray(ids == i).nonzero()\n",
    "            if len(hit_subject) == len(ids):\n",
    "                hit_count += 1\n",
    "        return hit_count / len(self.queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.losses import TripletDistanceMetric, TripletLoss\n",
    "\n",
    "\n",
    "def fine_tune(\n",
    "    model: SentenceTransformer, train_data: List[InputExample], val_data: List[Tuple[str, ...]],\n",
    "    output_path: str, batch_size: int, epochs: int,\n",
    "):\n",
    "    LOSS_MARGIN = 2  # Max value for cosine distance, which has a range of (0, 2)\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    train_loss = TripletLoss(model, distance_metric=TripletDistanceMetric.COSINE, triplet_margin=LOSS_MARGIN)\n",
    "    evaluator = CustomTripletEvaluator(\n",
    "        *map(list, zip(*val_data)),\n",
    "        SimilarityFunction.COSINE, LOSS_MARGIN, batch_size=batch_size,\n",
    "    )\n",
    "    evaluator(model, output_path=os.path.join(output_path, \"eval\"))\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=epochs,\n",
    "        warmup_steps=int(len(train_dataloader) * epochs * 0.1),\n",
    "        evaluator=evaluator,\n",
    "        output_path=output_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.5\n",
    "\n",
    "_train_items, val_items, train_data, val_data = generate_data(Item.fetch_items(), TRAIN_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "OUTPUT_PATH = \"./embedder/\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "\n",
    "model = SentenceTransformer(MODEL_ID)\n",
    "shutil.rmtree(OUTPUT_PATH, ignore_errors=True)\n",
    "old_hit_rate = SimilarityEvaluator(model, val_items, 10)(model)\n",
    "fine_tune(model, train_data, val_data, OUTPUT_PATH, BATCH_SIZE, EPOCHS)\n",
    "new_hit_rate = SimilarityEvaluator(model, val_items, 10)(model)\n",
    "print(f\"Hit rate: old={old_hit_rate}, new={new_hit_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
